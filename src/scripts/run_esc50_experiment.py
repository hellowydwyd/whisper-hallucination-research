"""
ESC-50 æ•°æ®é›†å¹»è§‰æµ‹è¯•å®éªŒ
========================

æµ‹è¯• Whisper åœ¨çœŸå®ç¯å¢ƒå£°éŸ³ä¸Šçš„å¹»è§‰ç°è±¡

ESC-50 åŒ…å« 50 ç±»ç¯å¢ƒå£°éŸ³ï¼š
- åŠ¨ç‰©: ç‹—å«ã€çŒ«å«ã€é¸Ÿé¸£ã€èŸ‹èŸ€ç­‰
- è‡ªç„¶: é›¨å£°ã€æµ·æµªã€é›·å£°ã€é£å£°ç­‰  
- äººç±»éè¯­éŸ³: å’³å—½ã€è„šæ­¥ã€ç¬‘å£°ç­‰
- å®¤å†…: é’Ÿå£°ã€é—¨é“ƒã€é”®ç›˜æ•²å‡»ç­‰
- åŸå¸‚: ç›´å‡æœºã€ç”µé”¯ã€è­¦ç¬›ç­‰

è¿è¡Œæ–¹å¼:
    conda activate d2l
    python run_esc50_experiment.py
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

import pandas as pd
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
from collections import Counter
import matplotlib.pyplot as plt

from config import DATA_DIR, OUTPUT_DIR, NON_SPEECH_DIR
from models.whisper_model import WhisperASR
from utils.audio_utils import load_audio, get_audio_duration
from utils.metrics import detect_looping, check_boh_match
from experiments.boh_filter import BoHFilter, postprocess_transcription
from config import COMMON_HALLUCINATIONS


# ESC-50 ç±»åˆ«æ˜ å°„
ESC50_CATEGORIES = {
    # åŠ¨ç‰© (0-9)
    0: 'dog', 1: 'rooster', 2: 'pig', 3: 'cow', 4: 'frog',
    5: 'cat', 6: 'hen', 7: 'insects', 8: 'sheep', 9: 'crow',
    # è‡ªç„¶å£°éŸ³ (10-19)
    10: 'rain', 11: 'sea_waves', 12: 'crackling_fire', 13: 'crickets', 14: 'chirping_birds',
    15: 'water_drops', 16: 'wind', 17: 'pouring_water', 18: 'toilet_flush', 19: 'thunderstorm',
    # äººç±»éè¯­éŸ³ (20-29)
    20: 'crying_baby', 21: 'sneezing', 22: 'clapping', 23: 'breathing', 24: 'coughing',
    25: 'footsteps', 26: 'laughing', 27: 'brushing_teeth', 28: 'snoring', 29: 'drinking_sipping',
    # å®¤å†…/å®¶åº­ (30-39)
    30: 'door_knock', 31: 'mouse_click', 32: 'keyboard_typing', 33: 'door_wood_creaks', 34: 'can_opening',
    35: 'washing_machine', 36: 'vacuum_cleaner', 37: 'clock_alarm', 38: 'clock_tick', 39: 'glass_breaking',
    # åŸå¸‚/å®¤å¤– (40-49)
    40: 'helicopter', 41: 'chainsaw', 42: 'siren', 43: 'car_horn', 44: 'engine',
    45: 'train', 46: 'church_bells', 47: 'airplane', 48: 'fireworks', 49: 'hand_saw',
}

ESC50_SUPER_CATEGORIES = {
    'animals': list(range(0, 10)),
    'natural': list(range(10, 20)),
    'human_non_speech': list(range(20, 30)),
    'interior': list(range(30, 40)),
    'exterior': list(range(40, 50)),
}


def get_esc50_metadata(audio_dir: Path) -> pd.DataFrame:
    """
    è§£æ ESC-50 æ–‡ä»¶åè·å–å…ƒæ•°æ®
    
    æ–‡ä»¶åæ ¼å¼: {fold}-{clip_id}-{take}-{target}.wav
    ä¾‹å¦‚: 1-100032-A-0.wav
    """
    audio_files = list(audio_dir.glob("*.wav"))
    
    metadata = []
    for f in audio_files:
        parts = f.stem.split('-')
        if len(parts) == 4:
            fold, clip_id, take, target = parts
            target = int(target)
            metadata.append({
                'file': f.name,
                'path': str(f),
                'fold': int(fold),
                'clip_id': clip_id,
                'take': take,
                'target': target,
                'category': ESC50_CATEGORIES.get(target, 'unknown'),
            })
    
    df = pd.DataFrame(metadata)
    
    # æ·»åŠ è¶…ç±»åˆ«
    def get_super_category(target):
        for super_cat, targets in ESC50_SUPER_CATEGORIES.items():
            if target in targets:
                return super_cat
        return 'unknown'
    
    df['super_category'] = df['target'].apply(get_super_category)
    
    return df


def run_esc50_experiment(
    model_size: str = "large-v3",
    max_samples: int = None,
    sample_per_category: int = None
):
    """
    è¿è¡Œ ESC-50 æ•°æ®é›†å®éªŒ
    
    Args:
        model_size: Whisper æ¨¡å‹å¤§å°
        max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ŒNone ä¸ºå…¨éƒ¨
        sample_per_category: æ¯ä¸ªç±»åˆ«æµ‹è¯•çš„æ ·æœ¬æ•°ï¼ŒNone ä¸ºå…¨éƒ¨
    """
    print("\n" + "=" * 60)
    print("   ESC-50 ç¯å¢ƒå£°éŸ³å¹»è§‰æµ‹è¯•å®éªŒ")
    print("=" * 60)
    
    # æŸ¥æ‰¾ ESC-50 æ•°æ®ç›®å½•
    esc50_dir = NON_SPEECH_DIR / "esc50"
    audio_dir = esc50_dir / "audio"
    
    if not audio_dir.exists():
        # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
        possible_paths = [
            esc50_dir / "ESC-50-master" / "audio",
            esc50_dir / "audio",
            NON_SPEECH_DIR / "ESC-50-master" / "audio",
        ]
        for p in possible_paths:
            if p.exists():
                audio_dir = p
                break
    
    if not audio_dir.exists():
        print(f"[Error] æ‰¾ä¸åˆ° ESC-50 éŸ³é¢‘ç›®å½•")
        print(f"[Error] è¯·å…ˆè¿è¡Œ: python download_datasets.py --dataset esc50")
        return None
    
    print(f"[ESC-50] éŸ³é¢‘ç›®å½•: {audio_dir}")
    
    # è·å–å…ƒæ•°æ®
    metadata = get_esc50_metadata(audio_dir)
    print(f"[ESC-50] æ€»éŸ³é¢‘æ•°: {len(metadata)}")
    print(f"[ESC-50] ç±»åˆ«æ•°: {metadata['target'].nunique()}")
    
    # é‡‡æ ·
    if sample_per_category:
        # æ¯ä¸ªç±»åˆ«é‡‡æ ·æŒ‡å®šæ•°é‡
        sampled = metadata.groupby('target').apply(
            lambda x: x.sample(min(len(x), sample_per_category), random_state=42)
        ).reset_index(drop=True)
        metadata = sampled
        print(f"[ESC-50] é‡‡æ ·å: {len(metadata)} ä¸ªæ ·æœ¬ (æ¯ç±» {sample_per_category} ä¸ª)")
    
    if max_samples and len(metadata) > max_samples:
        metadata = metadata.sample(max_samples, random_state=42)
        print(f"[ESC-50] é™åˆ¶æ ·æœ¬æ•°: {max_samples}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    print(f"\n[Model] åŠ è½½ Whisper {model_size}...")
    asr = WhisperASR(model_size=model_size)
    
    # åˆå§‹åŒ– BoH è¿‡æ»¤å™¨
    boh = BoHFilter()
    
    # è¿è¡Œå®éªŒ
    results = []
    
    print(f"\n[Experiment] å¼€å§‹æµ‹è¯• {len(metadata)} ä¸ªéŸ³é¢‘...")
    
    for idx, row in tqdm(metadata.iterrows(), total=len(metadata), desc="å¤„ç†ä¸­"):
        try:
            # è½¬å½•
            result = asr.transcribe(row['path'])
            transcription = result['text']
            
            # åå¤„ç†
            processed = postprocess_transcription(transcription)
            
            # åˆ†æ
            record = {
                'file': row['file'],
                'category': row['category'],
                'super_category': row['super_category'],
                'target': row['target'],
                'transcription': transcription,
                'processed': processed,
                'is_hallucination': len(transcription.strip()) > 0,
                'is_looping': detect_looping(transcription),
                'boh_matches': check_boh_match(transcription, COMMON_HALLUCINATIONS),
                'char_count': len(transcription),
                'word_count': len(transcription.split()),
            }
            results.append(record)
            
        except Exception as e:
            print(f"\n[Error] {row['file']}: {e}")
    
    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame(results)
    
    # ç»Ÿè®¡åˆ†æ
    print("\n" + "=" * 60)
    print("å®éªŒç»“æœç»Ÿè®¡")
    print("=" * 60)
    
    total = len(df)
    hallucination_count = df['is_hallucination'].sum()
    looping_count = df['is_looping'].sum()
    
    print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    print(f"  - æ€»æ ·æœ¬æ•°: {total}")
    print(f"  - å¹»è§‰æ•°: {hallucination_count}")
    print(f"  - å¹»è§‰ç‡: {hallucination_count/total:.1%}")
    print(f"  - å¾ªç¯ç‡: {looping_count/total:.1%}")
    
    # æŒ‰è¶…ç±»åˆ«ç»Ÿè®¡
    print(f"\nğŸ“Š æŒ‰å£°éŸ³ç±»å‹ç»Ÿè®¡:")
    super_stats = df.groupby('super_category').agg({
        'is_hallucination': ['sum', 'mean', 'count']
    }).round(3)
    super_stats.columns = ['å¹»è§‰æ•°', 'å¹»è§‰ç‡', 'æ ·æœ¬æ•°']
    print(super_stats.to_string())
    
    # æŒ‰å…·ä½“ç±»åˆ«ç»Ÿè®¡
    print(f"\nğŸ“Š å„ç±»åˆ«å¹»è§‰ç‡ (Top 10 æœ€é«˜):")
    category_stats = df.groupby('category')['is_hallucination'].agg(['sum', 'mean', 'count'])
    category_stats.columns = ['å¹»è§‰æ•°', 'å¹»è§‰ç‡', 'æ ·æœ¬æ•°']
    category_stats = category_stats.sort_values('å¹»è§‰ç‡', ascending=False)
    print(category_stats.head(10).to_string())
    
    # å¹»è§‰å†…å®¹åˆ†æ
    hallucinations = df[df['is_hallucination']]['transcription'].tolist()
    if hallucinations:
        print(f"\nğŸ“Š å¹»è§‰å†…å®¹åˆ†æ:")
        phrase_freq = Counter(hallucinations)
        print(f"  - å”¯ä¸€å¹»è§‰æ•°: {len(phrase_freq)}")
        print(f"  - Top 10 å¸¸è§å¹»è§‰:")
        for phrase, count in phrase_freq.most_common(10):
            pct = count / len(hallucinations) * 100
            display = phrase[:50] + "..." if len(phrase) > 50 else phrase
            print(f"      '{display}' ({count}æ¬¡, {pct:.1f}%)")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # CSV
    csv_path = OUTPUT_DIR / f"esc50_results_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\n[Save] CSV: {csv_path}")
    
    # JSON æ‘˜è¦
    summary = {
        'model': model_size,
        'dataset': 'ESC-50',
        'total_samples': total,
        'hallucination_count': int(hallucination_count),
        'hallucination_rate': float(hallucination_count / total),
        'looping_rate': float(looping_count / total),
        'by_super_category': {
            cat: {
                'count': int(stats['count']),
                'hallucination_rate': float(stats['mean'])
            }
            for cat, stats in df.groupby('super_category')['is_hallucination'].agg(['mean', 'count']).iterrows()
        },
        'top_hallucinations': phrase_freq.most_common(20) if hallucinations else [],
    }
    
    json_path = OUTPUT_DIR / f"esc50_summary_{timestamp}.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"[Save] JSON: {json_path}")
    
    # ç”Ÿæˆå¯è§†åŒ–
    generate_esc50_visualizations(df, OUTPUT_DIR, timestamp)
    
    return df, summary


def generate_esc50_visualizations(df: pd.DataFrame, output_dir: Path, timestamp: str):
    """
    ç”Ÿæˆ ESC-50 å®éªŒå¯è§†åŒ–
    """
    print("\n[å¯è§†åŒ–] ç”Ÿæˆå›¾è¡¨...")
    
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # å›¾1: æŒ‰è¶…ç±»åˆ«çš„å¹»è§‰ç‡
    ax1 = axes[0, 0]
    super_stats = df.groupby('super_category')['is_hallucination'].mean().sort_values(ascending=True)
    colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(super_stats)))
    bars = ax1.barh(super_stats.index, super_stats.values, color=colors)
    ax1.set_xlabel('Hallucination Rate')
    ax1.set_title('Hallucination Rate by Sound Category', fontsize=12, fontweight='bold')
    ax1.set_xlim(0, 1)
    for bar, val in zip(bars, super_stats.values):
        ax1.text(val + 0.02, bar.get_y() + bar.get_height()/2, f'{val:.1%}', va='center')
    
    # å›¾2: æŒ‰å…·ä½“ç±»åˆ«çš„å¹»è§‰ç‡ (Top 15)
    ax2 = axes[0, 1]
    cat_stats = df.groupby('category')['is_hallucination'].mean().sort_values(ascending=False).head(15)
    colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(cat_stats)))
    bars = ax2.barh(range(len(cat_stats)), cat_stats.values, color=colors)
    ax2.set_yticks(range(len(cat_stats)))
    ax2.set_yticklabels(cat_stats.index)
    ax2.set_xlabel('Hallucination Rate')
    ax2.set_title('Top 15 Categories by Hallucination Rate', fontsize=12, fontweight='bold')
    ax2.set_xlim(0, 1)
    ax2.invert_yaxis()
    
    # å›¾3: å¹»è§‰å†…å®¹é•¿åº¦åˆ†å¸ƒ
    ax3 = axes[1, 0]
    hall_df = df[df['is_hallucination']]
    if len(hall_df) > 0:
        ax3.hist(hall_df['char_count'], bins=30, color='#e74c3c', edgecolor='white', alpha=0.8)
        ax3.axvline(hall_df['char_count'].mean(), color='black', linestyle='--', label=f"Mean: {hall_df['char_count'].mean():.1f}")
        ax3.set_xlabel('Character Count')
        ax3.set_ylabel('Frequency')
        ax3.set_title('Distribution of Hallucination Length', fontsize=12, fontweight='bold')
        ax3.legend()
    
    # å›¾4: å¹»è§‰å†…å®¹è¯äº‘é£æ ¼çš„é¢‘ç‡å›¾
    ax4 = axes[1, 1]
    if len(hall_df) > 0:
        phrase_freq = Counter(hall_df['transcription'].tolist())
        top_phrases = phrase_freq.most_common(10)
        if top_phrases:
            phrases, counts = zip(*top_phrases)
            phrases = [p[:30] + "..." if len(p) > 30 else p for p in phrases]
            y_pos = range(len(phrases))
            colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(phrases)))
            ax4.barh(y_pos, counts, color=colors)
            ax4.set_yticks(y_pos)
            ax4.set_yticklabels(phrases)
            ax4.set_xlabel('Frequency')
            ax4.set_title('Top 10 Hallucination Phrases', fontsize=12, fontweight='bold')
            ax4.invert_yaxis()
    
    plt.tight_layout()
    
    fig_path = output_dir / f"esc50_results_{timestamp}.png"
    plt.savefig(fig_path, dpi=150, bbox_inches='tight')
    print(f"[å¯è§†åŒ–] å›¾è¡¨å·²ä¿å­˜: {fig_path}")
    
    plt.close()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ESC-50 æ•°æ®é›†å¹»è§‰æµ‹è¯•')
    parser.add_argument('--model', type=str, default='large-v3', help='Whisper æ¨¡å‹å¤§å°')
    parser.add_argument('--max-samples', type=int, default=None, help='æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--per-category', type=int, default=None, help='æ¯ç±»åˆ«æµ‹è¯•æ ·æœ¬æ•°')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæµ‹è¯•æ¨¡å¼ (æ¯ç±»5ä¸ª)')
    
    args = parser.parse_args()
    
    if args.quick:
        args.per_category = 5
        print("[Mode] å¿«é€Ÿæµ‹è¯•æ¨¡å¼: æ¯ç±»åˆ« 5 ä¸ªæ ·æœ¬")
    
    df, summary = run_esc50_experiment(
        model_size=args.model,
        max_samples=args.max_samples,
        sample_per_category=args.per_category
    )
    
    if summary:
        print("\n" + "=" * 60)
        print("ğŸ‰ ESC-50 å®éªŒå®Œæˆ!")
        print("=" * 60)
        print(f"  æ¨¡å‹: {summary['model']}")
        print(f"  æ ·æœ¬æ•°: {summary['total_samples']}")
        print(f"  å¹»è§‰ç‡: {summary['hallucination_rate']:.1%}")


if __name__ == "__main__":
    main()
